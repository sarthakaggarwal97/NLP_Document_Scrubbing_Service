{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Document_Scrubbing_Service",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdh9L0CReOXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ce5a0b-8198-4a6b-9e89-e97c3fcf2341"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "\n",
        "def scrubCommentary(commentary):\n",
        "  commentarySubSummary_text = decontracted(commentary['subscriptionSummary'])\n",
        "  commentarySubSummary_list = tokenize.sent_tokenize(commentarySubSummary_text)\n",
        "  scrub_commentary_main(commentarySubSummary_list, commentarySubSummary_text)\n",
        "\n",
        "  for pdfCommentary in commentary['attachedPDFContent']:\n",
        "    pdfCommentary_text = decontracted(pdfCommentary)\n",
        "    pdfCommentary_list = tokenize.sent_tokenize(pdfCommentary_text)\n",
        "    scrub_commentary_main(pdfCommentary_list, pdfCommentary_text)\n",
        "\n",
        "  commentaryHtmlContent_text = decontracted(commentary['htmlContent'])  \n",
        "  commentaryHtmlContent_list = tokenize.sent_tokenize(commentaryHtmlContent_text)\n",
        "  scrub_commentary_main(commentaryHtmlContent_list, commentaryHtmlContent_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmVEE1CIeGGh"
      },
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# from extractFromExcel import extractFromExcel\n",
        "# from nltk import tokenize\n",
        "\n",
        "# # Khyati's code will get us commentary\n",
        "# commentary = {}\n",
        "# commentary = extractFromExcel(0)\n",
        "\n",
        "# scrubCommentary(commentary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PodvsDzZhmwf"
      },
      "source": [
        "def getQList():\n",
        "  df = pd.ExcelFile('location/test.xlsx').parse('Sheet1') #you could add index_col=0 if there's an index\n",
        "  qlist=[]\n",
        "  qlist.append(df['A'])\n",
        "  return qlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bt_AQBv-vyV",
        "outputId": "fe4bd014-8f35-4fa4-a624-f3c9b3138b00"
      },
      "source": [
        "#HELPER FUNCTIONS\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "import numpy as np\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def check(word, x):\n",
        "    if word in x:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) \n",
        "\n",
        "## GET INDEX\n",
        "def getIndexOfSentenceFromText(qElement, text):\n",
        "  sentences = tokenize.sent_tokenize(text)\n",
        "  print(sentences)\n",
        "  indexList = []\n",
        "  sentenceList = []\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    if qElement in sentence:\n",
        "      ## CHECK FOR OUT OF BOUND INDEX IS PENDING, MIGHT NOT BE NEEDED\n",
        "      indexList.append(i)\n",
        "    i = i + 1\n",
        "  return indexList\n",
        "\n",
        "## SENTENCE SIMILARITY\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "def similarity(sent1, sent2):\n",
        "  query_vec = model.encode([sent1])[0]\n",
        "  sim = cosine(query_vec, model.encode([sent2])[0])\n",
        "  return sim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 20.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp37-none-any.whl size=103068 sha256=6968a0d3b02d005a5cab5d06c750ac2f96f649f06373ccacf954c2aebe6bd446\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f901fc3e7ef85a97c72e58a9e05793744926cf10126c50867f43e7d88eb43507\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:19<00:00, 20.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11GDVvQMyJfu",
        "outputId": "91019f18-c924-4c6a-d1fe-c381fee42779"
      },
      "source": [
        "!pip install spacy==2.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/0f/ca790def675011f25bce8775cf9002b5085cd2288f85e891f70b32c18752/spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 141kB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.41.1)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCO5YBsW_qOH",
        "outputId": "12f1b066-307f-41cf-afd3-6faaab03f5b4"
      },
      "source": [
        "!pip install neuralcoref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/6d/c90e5bfd1b8ef32f1b231a32f2f625bf33df7525324d2bbcd08992791d64/neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/f8/9505c0a9c21ec477631c8e7e377e006ec19280376160ce73a6b8d0702336/boto3-1.17.18-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.19.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Collecting botocore<1.21.0,>=1.20.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/42/5add1bc9e32717967c1f1cc7b36053f242c6a23bc36313549c4062e30efc/botocore-1.20.18-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 9.2MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.18->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.18->boto3->neuralcoref) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.18 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "Successfully installed boto3-1.17.18 botocore-1.20.18 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwOhzoB_AlGu",
        "outputId": "3418906b-984f-4cb0-a3d6-5da602d8c1b9"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases//download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 14kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->en-core-web-lg==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-core-web-lg==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->en-core-web-lg==2.1.0) (4.41.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp37-none-any.whl size=828255077 sha256=137f31d5cbf3a8c97ee16aefb14ac3d0ddfc6f9d2362f36fed8af03e9b3ba940\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/23/48/c3271dd3a62b4dbe0edc676eca71ca861cf8d985675438d3dc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiUxahM8j-En"
      },
      "source": [
        "def strike(text):\n",
        "    result = '\\u0336' + '\\u0336'.join(text)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD2O2NOo-_Mq"
      },
      "source": [
        "def scrub_commentary_main(commentary, commentary_text):\n",
        "  # !pip install spacy==2.2.4\n",
        "  # !pip install neuralcoref\n",
        "\n",
        "  import spacy\n",
        "  from spacy import displacy\n",
        "  from collections import Counter\n",
        "  import en_core_web_lg\n",
        "\n",
        "  nlp = en_core_web_lg.load()\n",
        "  doc = nlp(commentary_text) \n",
        "\n",
        "  org_list=[]\n",
        "\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_=='ORG':\n",
        "      org_list.append(ent.text)\n",
        "\n",
        "  print(\"Org List in Commentary -------------------- \",org_list)\n",
        "\n",
        "  qlist = ['Microsoft','Google', 'c']\n",
        "  #qlist = getQList()\n",
        "  final_qlist=intersection(qlist, org_list)\n",
        "  print(\"Final Q list is -------------------- \",final_qlist )\n",
        "  import neuralcoref\n",
        "\n",
        "  nlp = en_core_web_lg.load()\n",
        "  neuralcoref.add_to_pipe(nlp)\n",
        "  doc = nlp(commentary_text)\n",
        "  qword_ticker = {'Google':'GOOGL', 'Microsoft':'MSFT'}\n",
        "\n",
        "  for word in final_qlist:\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print(\"NEXT ITERATION HAS STARTED FOR THE QWORD -------------------- \", word)\n",
        "    \n",
        "    commentary= commentary_text.split('.')\n",
        "    print(\"New commentary list is =============== \", commentary)\n",
        "    commentary_len=len(commentary)\n",
        "    qword=word\n",
        "    print(\"Analyzing the Q Word --------------------\", qword)\n",
        "\n",
        "    qword_sent_idx=getIndexOfSentenceFromText(qword, commentary_text)\n",
        "    print(\"Q word sentence indices are --------------------\", qword_sent_idx)\n",
        "\n",
        "    relevant_sent_idx=[]\n",
        "    threshold=0.5\n",
        "  \n",
        "    for idx in qword_sent_idx:\n",
        "      if (idx not in relevant_sent_idx):\n",
        "        relevant_sent_idx.append(idx)\n",
        "      if idx-1>=0 and (idx-1 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx-1])>=threshold:\n",
        "        relevant_sent_idx.append(idx-1)\n",
        "      if idx-2>=0 and (idx-2 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx-2])>=threshold:\n",
        "        relevant_sent_idx.append(idx-2)\n",
        "      if idx+1<commentary_len and (idx+1 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx+1])>=threshold:\n",
        "        relevant_sent_idx.append(idx+1)\n",
        "      if idx+2<commentary_len and (idx+2 not in relevant_sent_idx) and similarity(commentary[idx], commentary[idx+2])>=threshold:\n",
        "        relevant_sent_idx.append(idx+2)\n",
        "    print(\"Relevant Sentence Index -------------------- \", relevant_sent_idx)\n",
        "\n",
        "    relevant_text=\"\"\n",
        "\n",
        "    for idx in relevant_sent_idx:\n",
        "      relevant_text = relevant_text+'. ' + commentary[idx]\n",
        "      \n",
        "    doc = nlp(relevant_text)\n",
        "\n",
        "    qword_cluster_list=[]\n",
        "    print(\"Clusters for the current commentary -------------------- \", doc._.coref_clusters)\n",
        "    for i in range(0 ,len(doc._.coref_clusters)):\n",
        "      if doc._.coref_clusters[i][0].text==qword:\n",
        "        print(doc._.coref_clusters[i].mentions)\n",
        "        \n",
        "        for word in doc._.coref_clusters[i].mentions:\n",
        "          qword_cluster_list.append(word.text)\n",
        "        print('Q Word Cluster List is --------------------', qword_cluster_list)\n",
        "          \n",
        "    scrub_list_idx=[]\n",
        "    for idx in relevant_sent_idx:\n",
        "      sentence=commentary[idx]\n",
        "      split_on_word=sentence.split(' ')\n",
        "      print(split_on_word)\n",
        "      for word in split_on_word:\n",
        "        if check(word, qword_cluster_list) or word==qword or word==qword_ticker[qword]:\n",
        "          scrub_list_idx.append(idx)\n",
        "          print('Scrubbed at word -------------------- ', word)\n",
        "          break\n",
        "        \n",
        "    new_commentary=\"\"\n",
        "    scrubbed_text=\"\"\n",
        "    \n",
        "\n",
        "    for idx in range(0,commentary_len):\n",
        "      if idx not in scrub_list_idx:\n",
        "        new_commentary = new_commentary+'. ' + commentary[idx]\n",
        "        scrubbed_text = scrubbed_text+'. '  + commentary[idx]\n",
        "      else:\n",
        "        scrubbed_text = scrubbed_text+'. '  + strike(commentary[idx])\n",
        "    \n",
        "    print(\"Original Text is -------------------- \", commentary)\n",
        "    print(\"Scrubbed Text is -------------------- \", scrubbed_text)\n",
        "      \n",
        "    commentary_text = new_commentary\n",
        "\n",
        "    print(\"New commentary text is -------------------- \", commentary_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLF3592Y-mP1"
      },
      "source": [
        "commentary='AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877. Microsoft\\'s direct competitor attempt is a search engine called Bing. Yahoo is almost at its closure. At Merck, we\\'re following the science to tackle some of the world\\'s greatest health threats. Get a glimpse of how we work to improve lives. MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines. Its features are similar in look, feel, and function to Google\\'s search. It hasn\\'t boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google\\'s whopping 65 percent [source: Nielsen]. It\\'s still not close, even if you add in Microsoft\\'s new 10-year search engine partner Yahoo, at 14 percent. The PG&E is an American investor-owned utility (IOU) with publicly traded stock.'\n",
        "commentary_text='AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877. Microsoft\\'s direct competitor attempt is a search engine called Bing. Yahoo is almost at its closure. At Merck, we\\'re following the science to tackle some of the world\\'s greatest health threats. Get a glimpse of how we work to improve lives. MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines. Its features are similar in look, feel, and function to Google\\'s search. It hasn\\'t boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google\\'s whopping 65 percent [source: Nielsen]. It\\'s still not close, even if you add in Microsoft\\'s new 10-year search engine partner Yahoo, at 14 percent. The PG&E is an American investor-owned utility (IOU) with publicly traded stock.'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA4zsSWWJ7g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0337adf3-0907-4c66-f888-a343ab55dc73"
      },
      "source": [
        "print(commentary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877. Microsoft's direct competitor attempt is a search engine called Bing. Yahoo is almost at its closure. At Merck, we're following the science to tackle some of the world's greatest health threats. Get a glimpse of how we work to improve lives. MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines. Its features are similar in look, feel, and function to Google's search. It hasn't boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google's whopping 65 percent [source: Nielsen]. It's still not close, even if you add in Microsoft's new 10-year search engine partner Yahoo, at 14 percent. The PG&E is an American investor-owned utility (IOU) with publicly traded stock.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKB5uDcQlvic",
        "outputId": "e5f1e7a8-590c-4f85-b48f-856bc0e2ee12"
      },
      "source": [
        "scrub_commentary_main(tokenize.sent_tokenize(decontracted(commentary)), decontracted(commentary_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Org List in Commentary --------------------  ['AT&T', 'Southwestern Bell Telephone Company', 'the Bell Telephone Company', 'Microsoft', 'Bing', 'Yahoo', 'Merck', 'MSFT', 'Bing', 'Live Search', 'MSN', 'Google', 'Nielsen', 'Microsoft', 'Bing', 'Live Search', 'MSN', 'Google', 'Microsoft', 'Yahoo', 'PG&E', 'IOU']\n",
            "Final Q list is --------------------  ['Microsoft', 'Google']\n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "NEXT ITERATION HAS STARTED FOR THE QWORD --------------------  Microsoft\n",
            "New commentary list is ===============  ['AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877', ' Microsoft is direct competitor attempt is a search engine called Bing', ' Yahoo is almost at its closure', ' At Merck, we are following the science to tackle some of the world is greatest health threats', ' Get a glimpse of how we work to improve lives', ' MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines', ' Its features are similar in look, feel, and function to Google is search', ' It has not boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google is whopping 65 percent [source: Nielsen]', ' It is still not close, even if you add in Microsoft is new 10-year search engine partner Yahoo, at 14 percent', ' The PG&E is an American investor-owned utility (IOU) with publicly traded stock', '']\n",
            "Analyzing the Q Word -------------------- Microsoft\n",
            "['AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877.', 'Microsoft is direct competitor attempt is a search engine called Bing.', 'Yahoo is almost at its closure.', 'At Merck, we are following the science to tackle some of the world is greatest health threats.', 'Get a glimpse of how we work to improve lives.', 'MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines.', 'Its features are similar in look, feel, and function to Google is search.', 'It has not boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google is whopping 65 percent [source: Nielsen].', 'It is still not close, even if you add in Microsoft is new 10-year search engine partner Yahoo, at 14 percent.', 'The PG&E is an American investor-owned utility (IOU) with publicly traded stock.']\n",
            "Q word sentence indices are -------------------- [1, 7, 8]\n",
            "Relevant Sentence Index --------------------  [1, 7, 5, 8]\n",
            "Clusters for the current commentary --------------------  [Microsoft: [Microsoft, It, Microsoft, Microsoft], Bing: [Bing, Bing], Live Search: [Live Search, Live Search], Nielsen].  MSFT: [Nielsen].  MSFT, its]]\n",
            "[Microsoft, It, Microsoft, Microsoft]\n",
            "Q Word Cluster List is -------------------- ['Microsoft', 'It', 'Microsoft', 'Microsoft']\n",
            "['', 'Microsoft', 'is', 'direct', 'competitor', 'attempt', 'is', 'a', 'search', 'engine', 'called', 'Bing']\n",
            "Scrubbed at word --------------------  Microsoft\n",
            "['', 'It', 'has', 'not', 'boomed', 'in', 'the', 'marketplace', 'yet,', 'though:', 'As', 'of', 'February', '2010,', 'Nielsen', 'reported', 'that', 'Microsoft', 'search', 'engines', '(Bing,', 'Live', 'Search,', 'and', 'MSN)', 'all', 'shared', 'a', 'mere', '12', 'percent', 'of', 'online', 'searches,', 'nowhere', 'near', 'Google', 'is', 'whopping', '65', 'percent', '[source:', 'Nielsen]']\n",
            "Scrubbed at word --------------------  It\n",
            "['', 'MSFT', 'launched', 'Bing', 'in', '2009', 'as', 'a', 'step', 'above', 'its', 'Live', 'Search', 'and', 'MSN', 'search', 'engines']\n",
            "Scrubbed at word --------------------  MSFT\n",
            "['', 'It', 'is', 'still', 'not', 'close,', 'even', 'if', 'you', 'add', 'in', 'Microsoft', 'is', 'new', '10-year', 'search', 'engine', 'partner', 'Yahoo,', 'at', '14', 'percent']\n",
            "Scrubbed at word --------------------  It\n",
            "Original Text is --------------------  ['AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877', ' Microsoft is direct competitor attempt is a search engine called Bing', ' Yahoo is almost at its closure', ' At Merck, we are following the science to tackle some of the world is greatest health threats', ' Get a glimpse of how we work to improve lives', ' MSFT launched Bing in 2009 as a step above its Live Search and MSN search engines', ' Its features are similar in look, feel, and function to Google is search', ' It has not boomed in the marketplace yet, though: As of February 2010, Nielsen reported that Microsoft search engines (Bing, Live Search, and MSN) all shared a mere 12 percent of online searches, nowhere near Google is whopping 65 percent [source: Nielsen]', ' It is still not close, even if you add in Microsoft is new 10-year search engine partner Yahoo, at 14 percent', ' The PG&E is an American investor-owned utility (IOU) with publicly traded stock', '']\n",
            "Scrubbed Text is --------------------  . AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877. ̶ ̶M̶i̶c̶r̶o̶s̶o̶f̶t̶ ̶i̶s̶ ̶d̶i̶r̶e̶c̶t̶ ̶c̶o̶m̶p̶e̶t̶i̶t̶o̶r̶ ̶a̶t̶t̶e̶m̶p̶t̶ ̶i̶s̶ ̶a̶ ̶s̶e̶a̶r̶c̶h̶ ̶e̶n̶g̶i̶n̶e̶ ̶c̶a̶l̶l̶e̶d̶ ̶B̶i̶n̶g.  Yahoo is almost at its closure.  At Merck, we are following the science to tackle some of the world is greatest health threats.  Get a glimpse of how we work to improve lives. ̶ ̶M̶S̶F̶T̶ ̶l̶a̶u̶n̶c̶h̶e̶d̶ ̶B̶i̶n̶g̶ ̶i̶n̶ ̶2̶0̶0̶9̶ ̶a̶s̶ ̶a̶ ̶s̶t̶e̶p̶ ̶a̶b̶o̶v̶e̶ ̶i̶t̶s̶ ̶L̶i̶v̶e̶ ̶S̶e̶a̶r̶c̶h̶ ̶a̶n̶d̶ ̶M̶S̶N̶ ̶s̶e̶a̶r̶c̶h̶ ̶e̶n̶g̶i̶n̶e̶s.  Its features are similar in look, feel, and function to Google is search. ̶ ̶I̶t̶ ̶h̶a̶s̶ ̶n̶o̶t̶ ̶b̶o̶o̶m̶e̶d̶ ̶i̶n̶ ̶t̶h̶e̶ ̶m̶a̶r̶k̶e̶t̶p̶l̶a̶c̶e̶ ̶y̶e̶t̶,̶ ̶t̶h̶o̶u̶g̶h̶:̶ ̶A̶s̶ ̶o̶f̶ ̶F̶e̶b̶r̶u̶a̶r̶y̶ ̶2̶0̶1̶0̶,̶ ̶N̶i̶e̶l̶s̶e̶n̶ ̶r̶e̶p̶o̶r̶t̶e̶d̶ ̶t̶h̶a̶t̶ ̶M̶i̶c̶r̶o̶s̶o̶f̶t̶ ̶s̶e̶a̶r̶c̶h̶ ̶e̶n̶g̶i̶n̶e̶s̶ ̶(̶B̶i̶n̶g̶,̶ ̶L̶i̶v̶e̶ ̶S̶e̶a̶r̶c̶h̶,̶ ̶a̶n̶d̶ ̶M̶S̶N̶)̶ ̶a̶l̶l̶ ̶s̶h̶a̶r̶e̶d̶ ̶a̶ ̶m̶e̶r̶e̶ ̶1̶2̶ ̶p̶e̶r̶c̶e̶n̶t̶ ̶o̶f̶ ̶o̶n̶l̶i̶n̶e̶ ̶s̶e̶a̶r̶c̶h̶e̶s̶,̶ ̶n̶o̶w̶h̶e̶r̶e̶ ̶n̶e̶a̶r̶ ̶G̶o̶o̶g̶l̶e̶ ̶i̶s̶ ̶w̶h̶o̶p̶p̶i̶n̶g̶ ̶6̶5̶ ̶p̶e̶r̶c̶e̶n̶t̶ ̶[̶s̶o̶u̶r̶c̶e̶:̶ ̶N̶i̶e̶l̶s̶e̶n̶]. ̶ ̶I̶t̶ ̶i̶s̶ ̶s̶t̶i̶l̶l̶ ̶n̶o̶t̶ ̶c̶l̶o̶s̶e̶,̶ ̶e̶v̶e̶n̶ ̶i̶f̶ ̶y̶o̶u̶ ̶a̶d̶d̶ ̶i̶n̶ ̶M̶i̶c̶r̶o̶s̶o̶f̶t̶ ̶i̶s̶ ̶n̶e̶w̶ ̶1̶0̶-̶y̶e̶a̶r̶ ̶s̶e̶a̶r̶c̶h̶ ̶e̶n̶g̶i̶n̶e̶ ̶p̶a̶r̶t̶n̶e̶r̶ ̶Y̶a̶h̶o̶o̶,̶ ̶a̶t̶ ̶1̶4̶ ̶p̶e̶r̶c̶e̶n̶t.  The PG&E is an American investor-owned utility (IOU) with publicly traded stock. \n",
            "New commentary text is --------------------  . AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877.  Yahoo is almost at its closure.  At Merck, we are following the science to tackle some of the world is greatest health threats.  Get a glimpse of how we work to improve lives.  Its features are similar in look, feel, and function to Google is search.  The PG&E is an American investor-owned utility (IOU) with publicly traded stock. \n",
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
            "NEXT ITERATION HAS STARTED FOR THE QWORD --------------------  Google\n",
            "New commentary list is ===============  ['', ' AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877', '  Yahoo is almost at its closure', '  At Merck, we are following the science to tackle some of the world is greatest health threats', '  Get a glimpse of how we work to improve lives', '  Its features are similar in look, feel, and function to Google is search', '  The PG&E is an American investor-owned utility (IOU) with publicly traded stock', ' ']\n",
            "Analyzing the Q Word -------------------- Google\n",
            "['.', 'AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877.', 'Yahoo is almost at its closure.', 'At Merck, we are following the science to tackle some of the world is greatest health threats.', 'Get a glimpse of how we work to improve lives.', 'Its features are similar in look, feel, and function to Google is search.', 'The PG&E is an American investor-owned utility (IOU) with publicly traded stock.']\n",
            "Q word sentence indices are -------------------- [5]\n",
            "Relevant Sentence Index --------------------  [5]\n",
            "Clusters for the current commentary --------------------  []\n",
            "['', '', 'Its', 'features', 'are', 'similar', 'in', 'look,', 'feel,', 'and', 'function', 'to', 'Google', 'is', 'search']\n",
            "Scrubbed at word --------------------  Google\n",
            "Original Text is --------------------  ['', ' AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877', '  Yahoo is almost at its closure', '  At Merck, we are following the science to tackle some of the world is greatest health threats', '  Get a glimpse of how we work to improve lives', '  Its features are similar in look, feel, and function to Google is search', '  The PG&E is an American investor-owned utility (IOU) with publicly traded stock', ' ']\n",
            "Scrubbed Text is --------------------  . .  AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877.   Yahoo is almost at its closure.   At Merck, we are following the science to tackle some of the world is greatest health threats.   Get a glimpse of how we work to improve lives. ̶ ̶ ̶I̶t̶s̶ ̶f̶e̶a̶t̶u̶r̶e̶s̶ ̶a̶r̶e̶ ̶s̶i̶m̶i̶l̶a̶r̶ ̶i̶n̶ ̶l̶o̶o̶k̶,̶ ̶f̶e̶e̶l̶,̶ ̶a̶n̶d̶ ̶f̶u̶n̶c̶t̶i̶o̶n̶ ̶t̶o̶ ̶G̶o̶o̶g̶l̶e̶ ̶i̶s̶ ̶s̶e̶a̶r̶c̶h.   The PG&E is an American investor-owned utility (IOU) with publicly traded stock.  \n",
            "New commentary text is --------------------  . .  AT&T began its history as Southwestern Bell Telephone Company, a subsidiary of the Bell Telephone Company, founded by Alexander Graham Bell in 1877.   Yahoo is almost at its closure.   At Merck, we are following the science to tackle some of the world is greatest health threats.   Get a glimpse of how we work to improve lives.   The PG&E is an American investor-owned utility (IOU) with publicly traded stock.  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW9Izkikw4NI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH3Fo41OqF3S"
      },
      "source": [
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
        "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SffzA-pZqJAR",
        "outputId": "6053e6dd-8a6c-48bb-d0f0-178324c8fb0e"
      },
      "source": [
        "print(split_into_sentences(\"Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer.', 'He also worked at craigslist.org as a business analyst.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxvs04EmqjLI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}